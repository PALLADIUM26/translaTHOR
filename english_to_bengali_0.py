# -*- coding: utf-8 -*-
"""English_to_Bengali_0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MpfBkUJUjc5YE4M5YzBItYa9vwtr332J
"""

!pip install transformers[torch] -U

!pip install transformers datasets

!pip install transformers huggingface_hub

# Load model directly
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-bn-en")
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-bn-en")

import pandas as pd
from datasets import Dataset

def load_data(english_file, bengali_file):
    with open(english_file, 'r', encoding='utf-8') as f:
        english_lines = f.readlines()
    with open(bengali_file, 'r', encoding='utf-8') as f:
        bengali_lines = f.readlines()

    data = {'translation': [{'en': en.strip(), 'bn': bn.strip()} for en, bn in zip(english_lines, bengali_lines)]}
    return Dataset.from_dict(data)

# Assuming 'train.en.txt' and 'train.bn.txt' are your training files
train_dataset = load_data('suparadev_en.txt', 'suparadev_bn.txt')
test_dataset = load_data('suparatest2018_en.txt', 'suparatest2018_bn.txt')

# Check the structure of the dataset
print(train_dataset)
print(test_dataset)

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
from torch.utils.data import DataLoader

# Try a different model. This one is publically available.
tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-bn-en")
# tokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-distilled-600M')

# Set the target language for the tokenizer
tokenizer.tgt_lang = 'ben' # Set to Bengali

def preprocess_function(examples):
    inputs = [ex['en'] for ex in examples['translation']]
    targets = [ex['bn'] for ex in examples['translation']]
    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True, padding=True)
    model_inputs = {k: [v for v in values if v is not None] for k, values in model_inputs.items()}
    return model_inputs

# train_dataset = train_dataset.map(preprocess_function, batched=True)
# test_dataset = test_dataset.map(preprocess_function, batched=True)

train_dataset.set_transform(preprocess_function)
test_dataset.set_transform(preprocess_function)

train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=1)

from transformers import AutoModelForSeq2SeqLM

# model = AutoModelForSeq2SeqLM.from_pretrained('facebook/nllb-200-distilled-600M')
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-bn-en")

# !pip install accelerate -U

from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer

training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,  # Enable mixed precision training
    gradient_accumulation_steps=4,  # Accumulate gradients
    remove_unused_columns=False # Tell the trainer to remove unused columns.
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
)

trainer.train()

metrics = trainer.evaluate()
print(metrics)

model.save_pretrained("./saved_model")
tokenizer.save_pretrained("./saved_model")

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load the saved model and tokenizer
model_dir = "./saved_model"
tokenizer = AutoTokenizer.from_pretrained(model_dir)
model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)

def translate_text(input_text):
    # Tokenize the input text
    inputs = tokenizer(input_text, return_tensors="pt", max_length=128, truncation=True)

    # Generate translation using the model
    outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)

    # Decode the generated text
    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return translated_text

# Get user input and generate translation
user_input = input("Enter the text to be translated from English to Bengali: ")
print(user_input)
translated_output = translate_text(user_input)
print("Translated text:", translated_output)

import zipfile
import os
from google.colab import files

def zip_folder(folder_path, output_path):
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(folder_path):
            for file in files:
                file_path = os.path.join(root, file)
                zipf.write(file_path, os.path.relpath(file_path, folder_path))

# Path to the folder you want to zip
folder_path = './saved_model'

# Path for the output zip file
output_filename = 'final0.zip'

# Create a zip file
zip_folder(folder_path, output_filename)

# Download the zip file
files.download(output_filename)