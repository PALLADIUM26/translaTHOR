# -*- coding: utf-8 -*-
"""Translate_EnglishToBangla.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/168psn5L_wZhmyDsaIqV9oTMb5MMxraeN

#Translating English to Bengali

##Importing libraries
"""

import torch
import transformers
import pandas as pd
from transformers import BertModel, BertConfig, AutoTokenizer, BertTokenizer
from torch.utils.data import DataLoader, TensorDataset
import csv

"""##Importing datasets"""

y_train = pd.read_csv('suparadev_bn.txt', delimiter='\t')
X_train = pd.read_csv('suparadev_en.txt', delimiter='\t')
y_test = pd.read_csv('suparatest2018_bn.txt', delimiter='\t')
X_test = pd.read_csv('suparatest2018_en.txt', delimiter='\t')
# print(X_train)

"""##Preprocessing"""

# train_set = []
# d2 = []

# for i in range(0, len(data1)):
#     d1.append([data1.iloc[i, 0], data2.iloc[i, 0]])
# print(d1)

# for i in range(0, len(data1)):
#     d1.append([data3.iloc[i, 0], data4.iloc[i, 0]])

# filename = '1.csv'
# with open(filename, 'w', newline='') as csvfile:
#     csvwriter = csv.writer(csvfile)
#     for i in d1:
#         csvwriter.writerows([d1.iloc[i, 0]])
# csvfile.close()

# filename = '2.csv'
# with open(filename, 'w', newline='') as csvfile:
#     csvwriter = csv.writer(csvfile)
#     for i in range(0, len(data3)):
#         csvwriter.writerows([data3.iloc[i, 0], data4.iloc[i, 0]])
# csvfile.close()

y_train_list = []
for i in range(len(y_train)):
    y_train_list.append(y_train.iloc[i,0])
# print(y_train_list)

X_train_list = []
for i in range(len(X_train)):
    X_train_list.append(X_train.iloc[i,0])
# print(X_train_list)

y_test_list = []
for i in range(len(y_test)):
    y_test_list.append(y_test.iloc[i,0])
# print(y_test_list)

X_test_list = []
for i in range(len(X_test)):
    X_test_list.append(X_test.iloc[i,0])
# print(X_test_list)

"""##Tokenization"""

# # Use a pipeline as a high-level helper
# from transformers import pipeline

# pipe = pipeline("fill-mask", model="sagorsarker/bangla-bert-base")

# # Load model directly
# from transformers import AutoTokenizer, AutoModelForMaskedLM

# tokenizer = AutoTokenizer.from_pretrained("sagorsarker/bangla-bert-base")
# model = AutoModelForMaskedLM.from_pretrained("sagorsarker/bangla-bert-base")

tokenizer_en = BertTokenizer.from_pretrained('bert-base-uncased')
tokenizer_bn = AutoTokenizer.from_pretrained("sagorsarker/bangla-bert-base")
# tokenizer2 = BertTokenizer.from_pretrained('bangla-bert-base')
# tokenizer1 = BertTokenizer(vocab_file='suparadev_bn.txt')
# tokenizer2 = BertTokenizer(vocab_file='suparadev_en.txt')
# tokenizer3 = BertTokenizer(vocab_file='suparatest2018_bn.txt')
# tokenizer4 = BertTokenizer(vocab_file='suparatest2018_en.txt')
# print(tokenizer)
# inputs = tokenizer_en(d1, padding=True, truncation=True, return_tensors="pt")
# print(type(X_train))
token_y_train = tokenizer_bn.batch_encode_plus(batch_text_or_text_pairs = y_train_list)
token_X_train = tokenizer_en.batch_encode_plus(batch_text_or_text_pairs = X_train_list)
token_y_test = tokenizer_bn.batch_encode_plus(batch_text_or_text_pairs = y_test_list)
token_X_test = tokenizer_en.batch_encode_plus(batch_text_or_text_pairs = X_test_list)

# print(dict_y_train)

"""##Initializing Model"""

config = BertConfig(vocab_size=30522, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)
model = BertModel(config)
# con

"""##Training Model"""



"""##Getting Output

##Save Model
"""